{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5163c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef6e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pyperclip.paste()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad5a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theorem 8.3.1 is useful in testing for correlation between two time series. If one of the two processes in the theorem is white noise, then it follows at once from the theorem that ˆρ12(h) is approximately normally distributed with mean 0 and variance 1/n, in which case it is straightforward to test the hypothesis that ρ12(h) = 0. However, if neither process iswhite noise, then a value of ˆρ12(h) that is large relative to n−1/2 does not necessarily indicate that ρ12(h) is different from zero. For example, suppose that {Xt1} and {Xt2} are two independent AR(1) processes with ρ11(h) = ρ22(h) = 0.8|h|. Then the large-sample variance of ρˆ12(h) is n−1  1 + 2 ∞ k=1(0.64)k  = 4.556n−1. It would therefore not be surprising to observe a value of ˆρ12(h) as large as 3n−1/2 even though {Xt1} and {Xt2} are independent. If on the other hand, ρ11(h) = 0.8|h| and ρ22(h) = (−0.8) |h|, then the large-sample variance of ˆρ12(h) is 0.2195n−1, and an observed value of 3n−1/2 for ˆρ12(h) would be very unlikely.\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(text.splitlines())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96a5e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.sub(r'\\.\\s*([A-Z])', r'.\\n\\1', text)\n",
    "result = re.sub(r'\\s{2,}', ' ', result)\n",
    "result =re.sub(r'\\s\\.','.', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19475d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theorem 8.3.1 is useful in testing for correlation between two time series.\n",
      "If one of the two processes in the theorem is white noise, then it follows at once from the theorem that ˆρ12(h) is approximately normally distributed with mean 0 and variance 1/n, in which case it is straightforward to test the hypothesis that ρ12(h) = 0.\n",
      "However, if neither process iswhite noise, then a value of ˆρ12(h) that is large relative to n−1/2 does not necessarily indicate that ρ12(h) is different from zero.\n",
      "For example, suppose that {Xt1} and {Xt2} are two independent AR(1) processes with ρ11(h) = ρ22(h) = 0.8|h|.\n",
      "Then the large-sample variance of ρˆ12(h) is n−1 1 + 2 ∞ k=1(0.64)k = 4.556n−1.\n",
      "It would therefore not be surprising to observe a value of ˆρ12(h) as large as 3n−1/2 even though {Xt1} and {Xt2} are independent.\n",
      "If on the other hand, ρ11(h) = 0.8|h| and ρ22(h) = (−0.8) |h|, then the large-sample variance of ˆρ12(h) is 0.2195n−1, and an observed value of 3n−1/2 for ˆρ12(h) would be very unlikely.\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "pyperclip.copy(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4952f920-f2d1-4e6f-9bb4-ab64d529a50b",
   "metadata": {},
   "source": [
    "In the following sections, we analyze the design choices in our\n",
    "architecture. The remaining results in this paper are based on\n",
    "the WMT’14 English-German task with 13 encoder layers at\n",
    "kernel size 3 and 5 decoder layers at kernel size 5. We use a\n",
    "target vocabulary of 160K words as well as vocabulary selection\n",
    "(Mi et al., 2016; L’Hostis et al., 2016) to decrease the size of\n",
    "the output layer which speeds up training and testing. The\n",
    "average vocabulary size for each training batch is about 20K target\n",
    "words. All figures are averaged over three runs (§4) and BLEU\n",
    "is reported on newstest2014 before unknown word replacement.\n",
    "We start with an experiment that removes the position embeddings from the encoder and decoder (§3.1). These embeddings\n",
    "allow our model to identify which portion of the source and\n",
    "target sequence it is dealing with but also impose a restriction\n",
    "on the maximum sentence length. Table 4 shows that position\n",
    "embeddings are helpful but that our model still performs well\n",
    "without them. Removing the source position embeddings results\n",
    "in a larger accuracy decrease than target position embeddings.\n",
    "However, removing both source and target positions decreases\n",
    "accuracy only by 0.5 BLEU. We also find that the length of the\n",
    "outputs of models without position embeddings closely matches\n",
    "the output length of models with position information. This\n",
    "indicates that the models can learn relative position information\n",
    "within the contexts visible to the encoder and decoder networks\n",
    "which can observe up to 27 and 25 words respectively.\n",
    "Recurrent models typically do not use explicit position\n",
    "embeddings since they can learn where they are in the sequence\n",
    "through the recurrent hidden state computation. In our setting,\n",
    "the use of position embeddings requires only a simple addition\n",
    "to the input word embeddings which is a negligible overhead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "deepl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
